{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from statsmodels.robust import mad\n",
    "import random\n",
    "import cv2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "imsize = (128, 128, 3)\n",
    "latentSize = 64\n",
    "\n",
    "#encoder_type deepmind_enc\n",
    "data_dir = '/home/dovi/home/dataset'\n",
    "graph_dir = '/content/drive/My Drive/Colab Notebooks/VAE/graphs'\n",
    "test_image_folder = '/content/drive/My Drive/Colab Notebooks/VAE/test_images/celeb'\n",
    "encoder_type = 'deepmind_enc'\n",
    "save_dir = '/content/drive/My Drive/Colab Notebooks/VAE/saved_models'\n",
    "save_dir_img = '/content/drive/My Drive/Colab Notebooks/VAE/new_vae_images'\n",
    "val_split = 0.2\n",
    "\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32 \n",
    "optimizer = 'ADAM' \n",
    "base_learning_rate = 0.01\n",
    "num_epochs = 100 \n",
    "scheduler_epoch = 5 \n",
    "decay_factor = 0.01 \n",
    "capacity = 25\n",
    "multi_process=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "numdim = 30 # Number of latent dimensions\n",
    "\n",
    "# Rest of this cell uses the loss adding method from https://www.kaggle.com/rvislaywade/visualizing-mnist-using-a-variational-autoencoder\n",
    "def sample(args):\n",
    "    z_mu, z_log_sigma = args\n",
    "    epsilon = K.random_normal((K.shape(z_mu)[0], numdim),)\n",
    "    return z_mu + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "class CustomLossLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n",
    "        \n",
    "        return K.mean(xent_loss + kl_loss)        \n",
    "\n",
    "    # Adds the custom loss\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return z_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "AUTOENCODER ARCHITECTURE (\"model\" is the decoder)\n",
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 64, 64, 32)   1568        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 32, 32, 64)   32832       conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 16, 16, 128)  131200      conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 16, 16, 128)  262272      conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 128)  262272      conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 16, 256)  524544      conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 16, 16, 256)  1048832     conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 16, 16, 256)  262400      conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 65536)        0           conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 30)           1966110     flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 30)           1966110     flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 30)           0           dense_42[0][0]                   \n",
      "                                                                 dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 30)           120         lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_39 (Model)                (None, 128, 128, 3)  6407888     batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "custom_loss_layer_12 (CustomLos (None, 128, 128, 3)  0           input_7[0][0]                    \n",
      "                                                                 model_39[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,866,148\n",
      "Trainable params: 12,866,088\n",
      "Non-trainable params: 60\n",
      "__________________________________________________________________________________________________\n",
      "DECODER ARCHITECTURE\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 65536)             2031616   \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_132 (Conv2D (None, 16, 16, 256)       262400    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_133 (Conv2D (None, 16, 16, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_134 (Conv2D (None, 16, 16, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_135 (Conv2D (None, 16, 16, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_136 (Conv2D (None, 16, 16, 128)       524416    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_137 (Conv2D (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_138 (Conv2D (None, 16, 16, 64)        131136    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_139 (Conv2D (None, 16, 16, 32)        32800     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_140 (Conv2D (None, 32, 32, 24)        12312     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_141 (Conv2D (None, 64, 64, 9)         3465      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_142 (Conv2D (None, 128, 128, 3)       975       \n",
      "=================================================================\n",
      "Total params: 6,407,888\n",
      "Trainable params: 6,407,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(imsize)\n",
    "\n",
    "x = Conv2D(32, 4,\n",
    "          activation = 'relu',\n",
    "          strides = 2,\n",
    "          padding = 'same')(input_img)\n",
    "\n",
    "x = Conv2D(64, 4,\n",
    "          activation = 'relu',\n",
    "          strides = 2,\n",
    "          padding = 'same')(x)\n",
    "\n",
    "x = Conv2D(128, 4,\n",
    "          activation = 'relu',\n",
    "          strides = 2,\n",
    "          padding = 'same')(x)\n",
    "\n",
    "x = Conv2D(128, 4,\n",
    "          activation='relu',\n",
    "          padding = 'same')(x)\n",
    "\n",
    "x = Conv2D(128, 4,\n",
    "          activation = 'relu',\n",
    "          padding = 'same')(x)\n",
    "\n",
    "x = Conv2D(256, 4,\n",
    "          activation = 'relu',\n",
    "          padding='same')(x)\n",
    "\n",
    "x = Conv2D(256, 4,\n",
    "          activation = 'relu',\n",
    "          padding = 'same')(x)\n",
    "\n",
    "x = Conv2D(256, (1,4),\n",
    "          activation = 'relu',\n",
    "          padding = 'same')(x)\n",
    "\n",
    "b4shape = K.int_shape(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "z_mu = Dense(numdim)(x)\n",
    "z_log_sigma = Dense(numdim)(x)\n",
    "z = Lambda(sample)([z_mu, z_log_sigma]) # Combining z_mu and z_log_sigma\n",
    "z = BatchNormalization()(z)\n",
    "\n",
    "encoder = Model(input_img, z) # Assigns half the VAE\n",
    "print('here')\n",
    "decoder_input = Input(K.int_shape(z)[1:])\n",
    "\n",
    "x = Dense(np.prod(b4shape[1:]),\n",
    "          activation = 'relu')(decoder_input)\n",
    "\n",
    "x = Reshape(b4shape[1:])(x)\n",
    "\n",
    "x = Conv2DTranspose(256, (4,1), \n",
    "                  activation = 'relu',\n",
    "                  padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(256, 4, \n",
    "                  activation = 'relu',\n",
    "                  padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(256, 4, \n",
    "                  activation = 'relu',\n",
    "                  padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(256, 4, \n",
    "                  activation = 'relu',\n",
    "                  padding = 'same')(x)\n",
    "\n",
    "x = Conv2DTranspose(128, 4, \n",
    "                  activation = 'relu',\n",
    "                  padding = 'same',)(x)\n",
    "\n",
    "x = Conv2DTranspose(128, 4,  \n",
    "                  activation = 'relu',\n",
    "                  padding = 'same')(x)\n",
    "\n",
    "x = Conv2DTranspose(64, 4,  \n",
    "                  activation = 'relu',\n",
    "                  padding = 'same')(x)\n",
    "\n",
    "x = Conv2DTranspose(32, 4,  \n",
    "                 activation = 'relu',\n",
    "                 padding = 'same')(x)\n",
    "\n",
    "x = Conv2DTranspose(24, 4,  \n",
    "                  activation = 'relu',\n",
    "                  padding = 'same',\n",
    "                   strides = 2)(x)\n",
    "\n",
    "x = Conv2DTranspose(9, 4,  \n",
    "                  activation = 'relu',\n",
    "                  padding = 'same',\n",
    "                  strides = 2)(x)\n",
    "\n",
    "x = Conv2DTranspose(3, 6, \n",
    "          padding = 'same', \n",
    "          activation = 'sigmoid',\n",
    "          strides = 2)(x)\n",
    "\n",
    "decoder = Model(decoder_input, x) # Assigns half the VAE\n",
    "\n",
    "z_decoded = decoder(z)\n",
    "\n",
    "y = CustomLossLayer()([input_img, z_decoded]) # Adds loss\n",
    "\n",
    "def no_loss(y_true, y_pred): # Because loss is given in the custom layer\n",
    "\n",
    "    return y_true-y_pred\n",
    "\n",
    "vae = Model(input_img, y) # Assigns the combined encoder-decoder model\n",
    "vae.compile(optimizer=Adam(lr=0.00146), loss=no_loss) \n",
    "\n",
    "plot_model(vae, 'vae_architecture.png', show_shapes = True) # Plots model architecture to a file\n",
    "plot_model(decoder, 'decoder_architecture.png', show_shapes = True)\n",
    "\n",
    "print('AUTOENCODER ARCHITECTURE (\"model\" is the decoder)')\n",
    "vae.summary()\n",
    "print('DECODER ARCHITECTURE')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights('vae_w_192_30.hdf5')\n",
    "\n",
    "vae.load_weights('vae_w_192_30.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 238996 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# image feeder for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir, # Folder name with all the images. Inside the folder should be ANOTHER folder with images\n",
    "        target_size=imsize[:-1],\n",
    "        batch_size=16,)\n",
    "\n",
    "training_steps = math.ceil(train_generator.n / 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [16,1] vs. [16,128,128,3]\n\t [[{{node loss_1/custom_loss_layer_12_loss/sub}}]]\n\t [[loss_1/add/_1349]]\n  (1) Invalid argument: Incompatible shapes: [16,1] vs. [16,128,128,3]\n\t [[{{node loss_1/custom_loss_layer_12_loss/sub}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6a4162a13f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3580\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3581\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3582\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [16,1] vs. [16,128,128,3]\n\t [[{{node loss_1/custom_loss_layer_12_loss/sub}}]]\n\t [[loss_1/add/_1349]]\n  (1) Invalid argument: Incompatible shapes: [16,1] vs. [16,128,128,3]\n\t [[{{node loss_1/custom_loss_layer_12_loss/sub}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "#vae.fit_generator(train_generator,steps_per_epoch=16054//16,epochs=10,)\n",
    "\n",
    "#training\n",
    "epochs = 0\n",
    "while epochs<10:\n",
    "    vae.fit_generator(train_generator,steps_per_epoch=training_steps,epochs=1,)\n",
    "    epochs +=1\n",
    "    print(epochs)\n",
    "    vae.save('vae_192_30.h5py')\n",
    "    vae.save_weights('vae_w_192_30.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_output(generated_image):\n",
    "    generated_image[generated_image > 0.5] = 0.5\n",
    "    generated_image[generated_image < -0.5] = -0.5\n",
    "    gen_image = np.uint8((generated_image + 0.5) * 255)\n",
    "    return gen_image\n",
    "\n",
    "\n",
    "def batch_gen(generator_object):\n",
    "    while True:\n",
    "        data = generator_object.next()\n",
    "        yield [pre_process_input(data[0])], [pre_process_input(data[0])]\n",
    "      \n",
    "      \n",
    "def pre_process_input(image_array):\n",
    "    x = np.asarray(image_array)\n",
    "    y = x - 0.5\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191197 images belonging to 1 classes.\n",
      "Found 47799 images belonging to 1 classes.\n",
      "Steps per epoch in training:  5975\n",
      "Steps per epoch in validation:  1494\n"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(rescale=1 / 255.,validation_split=val_split)\n",
    "\n",
    "train_generator = data_gen.flow_from_directory(data_dir, target_size=(image_size, image_size), batch_size=train_batch_size, class_mode='categorical', subset='training')\n",
    "\n",
    "validation_generator = data_gen.flow_from_directory(data_dir, target_size=(image_size, image_size), batch_size=val_batch_size, class_mode='categorical', subset='validation')\n",
    "\n",
    "training_steps = math.ceil(train_generator.n / train_batch_size)\n",
    "validation_steps = math.ceil(validation_generator.n / val_batch_size)\n",
    "\n",
    "print('Steps per epoch in training: ', training_steps)\n",
    "print('Steps per epoch in validation: ', validation_steps)\n",
    "\n",
    "training_generator = batch_gen(generator_object=train_generator)\n",
    "\n",
    "validation_generator = batch_gen(generator_object=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5975/5975 [==============================] - 2125s 356ms/step - loss: -0.9980 - val_loss: -0.9697\n",
      "Epoch 2/100\n",
      "5975/5975 [==============================] - 1458s 244ms/step - loss: -1.0011 - val_loss: -0.9696\n",
      "Epoch 3/100\n",
      "5975/5975 [==============================] - 1472s 246ms/step - loss: -1.0011 - val_loss: -0.9697\n",
      "Epoch 4/100\n",
      "5975/5975 [==============================] - 1473s 247ms/step - loss: -1.0011 - val_loss: -0.9700\n",
      "Epoch 5/100\n",
      "5975/5975 [==============================] - 1473s 247ms/step - loss: -1.0011 - val_loss: -0.9697\n",
      "Epoch 6/100\n",
      "5975/5975 [==============================] - 1476s 247ms/step - loss: -1.0011 - val_loss: -0.9700\n",
      "Epoch 7/100\n",
      "1051/5975 [====>.........................] - ETA: 16:19 - loss: -1.0006"
     ]
    }
   ],
   "source": [
    "vae.fit_generator(training_generator, steps_per_epoch=training_steps, epochs=num_epochs,validation_data=validation_generator,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save('vae_100_30.h5py')\n",
    " vae.save_weights('vae_w_100_30.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
